{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0d76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8965ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec735d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0033594",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d31f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ea157b3",
   "metadata": {},
   "source": [
    "## Exploration seq2seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1626154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "from torch.utils.data import Dataset\n",
    "import pdb\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b22ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "MAX_LOUDNESS = 127\n",
    "MAX_NOTE = 127\n",
    "\n",
    "with open(\"/home/zar3bski/Documents/Code/data/wavaetro/data/maestro-v3.0.0/2004_sym13.pickle\", \"rb\") as file: \n",
    "    df:DataFrame = pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16614422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbine/seq2seq-pytorch\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src):\n",
    "        # src : [sen_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        # embedded : [sen_len, batch_size, emb_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [sen_len, batch_size, hid_dim * n_directions]\n",
    "        # hidden = [n_layers * n_direction, batch_size, hid_dim]\n",
    "        # cell = [n_layers * n_direction, batch_size, hid_dim]\n",
    "        return hidden, cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4d5ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        # input = [batch_size]\n",
    "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
    "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        # input : [1, ,batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch_size, emb_dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [seq_len, batch_size, hid_dim * n_dir]\n",
    "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
    "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
    "        \n",
    "        # seq_len and n_dir will always be 1 in the decoder\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8970f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim\n",
    "        assert encoder.n_layers == decoder.n_layers\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, src:list, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [sen_len, batch_size]\n",
    "        # trg = [sen_len, batch_size]\n",
    "        # teacher_forcing_ratio : the probability to use the teacher forcing.\n",
    "        batch_size = len(src)\n",
    "        trg_len = len(trg)\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # ICI ça passe\n",
    "        \n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # first input to the decoder is the <sos> token.\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            # insert input token embedding, previous hidden and previous cell states \n",
    "            # receive output tensor (predictions) and new hidden and cell states.\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            # replace predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # decide if we are going to use teacher forcing or not.\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # get the highest predicted token from our predictions.\n",
    "            top1 = output.argmax(1)\n",
    "            # update input : use ground_truth when teacher_force \n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5db650",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MySet(Dataset): \n",
    "    def __init__(self, dataframe):\n",
    "        self.sym13 = dataframe[\"sym13\"]\n",
    "        self.midi_notes = dataframe[\"midi_notes\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sym13)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sym13 = self.sym13[idx]\n",
    "        midi_notes = self.midi_notes[idx]\n",
    "        return {\"sym13\":sym13, \"midi_notes\":midi_notes}\n",
    "\n",
    "def collate_wavelet(batch): \n",
    "    input = [torch.tensor(x[\"sym13\"]) for x in batch]\n",
    "    target = [torch.tensor(x[\"midi_notes\"]) for x in batch]\n",
    "    return input, target\n",
    "\n",
    "df.sample(35, replace=True)\n",
    "\n",
    "train_data =  MySet(df.sample(round(len(df)*0.6)).reset_index(inplace=False)) \n",
    "valid_data = MySet(df.sample(round(len(df)*0.4)).reset_index(inplace=False) )\n",
    "test_data = MySet(df.sample(round(len(df)*0.2)).reset_index(inplace=False) )\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "# We use a BucketIterator instead of the standard Iterator as it create batches in such a way that it minimizes the amount \n",
    "# of padding in both the source and target sentences.\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_wavelet)\n",
    "valid_iter = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=collate_wavelet)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=collate_wavelet)\n",
    "\n",
    "INPUT_DIM = len(df)\n",
    "OUTPUT_DIM = len(df)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7186f21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[-1.9520, -2.5907],\n",
       "          [ 4.0641, -0.9448],\n",
       "          [-0.6219, -1.9365],\n",
       "          ...,\n",
       "          [-1.5870,  0.2753],\n",
       "          [ 1.7409,  0.0081],\n",
       "          [-5.4818, -0.8124]], dtype=torch.float64),\n",
       "  tensor([[ 0.5840, -1.2531],\n",
       "          [ 2.5744, -0.0192],\n",
       "          [-6.2650, -1.2578],\n",
       "          ...,\n",
       "          [-0.4185, -0.7821],\n",
       "          [ 1.0146,  1.6577],\n",
       "          [ 0.8916, -2.9571]], dtype=torch.float64),\n",
       "  tensor([[-1.2517,  0.7547],\n",
       "          [-4.6351, -2.9741],\n",
       "          [ 9.8361,  8.0403],\n",
       "          ...,\n",
       "          [-0.2046,  0.2544],\n",
       "          [-1.0759,  0.0202],\n",
       "          [-1.0229,  1.3263]], dtype=torch.float64),\n",
       "  tensor([[  6.0274,  -6.3221],\n",
       "          [ 10.2162,  -0.7321],\n",
       "          [ -3.4835,  -3.7290],\n",
       "          ...,\n",
       "          [  7.6696,   3.1985],\n",
       "          [ -0.6773, -11.9212],\n",
       "          [  6.1902, -16.5063]], dtype=torch.float64),\n",
       "  tensor([[  1.6696,  -1.6760],\n",
       "          [ -2.6921,   2.2933],\n",
       "          [ -0.7913,   0.6379],\n",
       "          ...,\n",
       "          [  5.6176, -44.5511],\n",
       "          [-28.3669,  69.9069],\n",
       "          [ 76.0777, -57.4789]], dtype=torch.float64),\n",
       "  tensor([[-2.6923, -1.1101],\n",
       "          [-1.4347, -0.1881],\n",
       "          [ 0.4285,  0.1152],\n",
       "          ...,\n",
       "          [ 0.4522, -0.0119],\n",
       "          [ 1.1703,  0.5994],\n",
       "          [ 0.8273,  3.1031]], dtype=torch.float64),\n",
       "  tensor([[ 2.0494, -0.9311],\n",
       "          [ 4.5557,  3.0726],\n",
       "          [ 1.2885, -3.6340],\n",
       "          ...,\n",
       "          [ 3.0900, -0.5411],\n",
       "          [ 1.8296, -2.8003],\n",
       "          [-2.0477,  1.3634]], dtype=torch.float64),\n",
       "  tensor([[ -0.1982,   0.8984],\n",
       "          [  0.2030,  -0.1357],\n",
       "          [ -0.7612,   0.6423],\n",
       "          ...,\n",
       "          [ 39.9890,  27.8587],\n",
       "          [-42.9113, -24.3265],\n",
       "          [ 38.0186,  34.6337]], dtype=torch.float64),\n",
       "  tensor([[ 1.6534, -0.1713],\n",
       "          [-0.1482,  1.7871],\n",
       "          [ 0.2322, -0.3752],\n",
       "          ...,\n",
       "          [ 1.2186, -2.1076],\n",
       "          [ 0.7097,  0.8744],\n",
       "          [-1.2750, -1.3827]], dtype=torch.float64),\n",
       "  tensor([[-1.0082, -0.7410],\n",
       "          [-0.1252, -1.4978],\n",
       "          [ 0.7587,  1.5930],\n",
       "          ...,\n",
       "          [ 0.7420, -0.5867],\n",
       "          [-1.2442, -1.8754],\n",
       "          [ 1.3528,  0.7036]], dtype=torch.float64),\n",
       "  tensor([[ -2.6950,   1.8061],\n",
       "          [ -0.0745,   0.1566],\n",
       "          [  0.0988,   0.2147],\n",
       "          ...,\n",
       "          [ 26.8555, -10.6229],\n",
       "          [-21.1310,  23.3008],\n",
       "          [ -2.5513, -30.9483]], dtype=torch.float64),\n",
       "  tensor([[-0.1897, -0.5921],\n",
       "          [-3.5802,  0.3691],\n",
       "          [ 0.0807, -2.3781],\n",
       "          ...,\n",
       "          [-1.9382, -0.0519],\n",
       "          [ 0.6955,  0.4944],\n",
       "          [ 1.7726,  2.6559]], dtype=torch.float64),\n",
       "  tensor([[ 8.0264, -6.8722],\n",
       "          [-0.6677,  1.2832],\n",
       "          [-1.6809, -0.3660],\n",
       "          ...,\n",
       "          [-3.1565, -3.2773],\n",
       "          [-7.8019, -4.1516],\n",
       "          [ 3.2531,  1.6299]], dtype=torch.float64),\n",
       "  tensor([[ 1.4183, -0.0757],\n",
       "          [ 0.5656,  1.4661],\n",
       "          [ 3.3839, -0.7574],\n",
       "          ...,\n",
       "          [-0.5722, -4.9584],\n",
       "          [-6.3954,  5.7790],\n",
       "          [-1.0270, -2.5082]], dtype=torch.float64),\n",
       "  tensor([[-1.4606, -4.2283],\n",
       "          [ 0.2963,  3.1549],\n",
       "          [ 2.9059,  1.1953],\n",
       "          ...,\n",
       "          [ 2.0455, -0.0895],\n",
       "          [-0.9652,  1.1047],\n",
       "          [ 0.4475, -0.4853]], dtype=torch.float64),\n",
       "  tensor([[ 2.9367,  2.6001],\n",
       "          [ 1.6860,  7.3075],\n",
       "          [-0.1654, -3.7718],\n",
       "          ...,\n",
       "          [-0.3538, -2.0892],\n",
       "          [-4.4632,  1.4511],\n",
       "          [ 1.3041, -2.8029]], dtype=torch.float64),\n",
       "  tensor([[  0.9936,  -1.9471],\n",
       "          [ -0.7409,   0.6598],\n",
       "          [ -3.3426,   1.0428],\n",
       "          ...,\n",
       "          [-24.3158, -12.1029],\n",
       "          [ 30.1807,   2.9592],\n",
       "          [-15.4592,  -2.3564]], dtype=torch.float64),\n",
       "  tensor([[ 2.0108, -0.3400],\n",
       "          [-2.7562, -0.3065],\n",
       "          [-0.3341, -0.6870],\n",
       "          ...,\n",
       "          [-1.7413,  0.6215],\n",
       "          [ 1.0846,  0.5039],\n",
       "          [-0.0803, -0.7194]], dtype=torch.float64),\n",
       "  tensor([[ 0.0234, -2.0016],\n",
       "          [-0.5491,  1.6106],\n",
       "          [-0.7072, -1.4206],\n",
       "          ...,\n",
       "          [ 0.9695,  0.8880],\n",
       "          [-0.2960, -3.6010],\n",
       "          [-0.6499, -0.2723]], dtype=torch.float64),\n",
       "  tensor([[ 0.6959, -0.4935],\n",
       "          [ 0.1788, -1.5199],\n",
       "          [ 4.1557,  1.8331],\n",
       "          ...,\n",
       "          [-0.3011, -0.2369],\n",
       "          [ 1.8125, -0.2519],\n",
       "          [ 1.7991, -0.2993]], dtype=torch.float64),\n",
       "  tensor([[ 0.3510, -3.4968],\n",
       "          [-1.5866,  1.3739],\n",
       "          [ 1.5877, -0.1469],\n",
       "          ...,\n",
       "          [ 0.5623, -1.0103],\n",
       "          [-3.1484, -0.5924],\n",
       "          [ 2.8899, -1.1926]], dtype=torch.float64),\n",
       "  tensor([[ 3.7172, -0.1478],\n",
       "          [-0.8759,  1.2641],\n",
       "          [ 1.1173, -2.4142],\n",
       "          ...,\n",
       "          [-0.0302, -0.0311],\n",
       "          [-2.6439, -0.6986],\n",
       "          [ 3.6664, -0.4186]], dtype=torch.float64),\n",
       "  tensor([[ -0.3452,  -0.7985],\n",
       "          [ -0.1251,  -0.6725],\n",
       "          [  1.3632,   0.5419],\n",
       "          ...,\n",
       "          [-27.7860, -28.3313],\n",
       "          [ 17.1140,  51.0238],\n",
       "          [ -5.3866, -63.2257]], dtype=torch.float64),\n",
       "  tensor([[ 3.0654,  4.7771],\n",
       "          [-3.8316, -1.1564],\n",
       "          [ 0.7781,  4.4614],\n",
       "          ...,\n",
       "          [ 0.2444, -1.0482],\n",
       "          [ 1.0870,  0.1671],\n",
       "          [ 2.1816,  5.0639]], dtype=torch.float64),\n",
       "  tensor([[  1.3439,   0.4676],\n",
       "          [  1.1106,  -1.8248],\n",
       "          [  0.9807,  -1.0998],\n",
       "          ...,\n",
       "          [  4.4922, -52.3331],\n",
       "          [ 15.6568,  57.2068],\n",
       "          [-12.5554, -75.0496]], dtype=torch.float64),\n",
       "  tensor([[ 3.8965, -0.5267],\n",
       "          [ 1.0396,  1.0284],\n",
       "          [-3.9037, -3.0422],\n",
       "          ...,\n",
       "          [-0.4713,  3.9072],\n",
       "          [ 0.4263,  1.2141],\n",
       "          [-1.6407,  0.4584]], dtype=torch.float64),\n",
       "  tensor([[ 0.0901,  0.7321],\n",
       "          [ 0.3268,  0.0471],\n",
       "          [-0.6324, -1.9184],\n",
       "          ...,\n",
       "          [-2.4539,  0.1188],\n",
       "          [-0.4895, -0.1206],\n",
       "          [-1.4878, -1.1650]], dtype=torch.float64),\n",
       "  tensor([[-1.5791,  1.4408],\n",
       "          [-2.4009,  0.2967],\n",
       "          [-2.2534, -0.9564],\n",
       "          ...,\n",
       "          [ 0.8622, -2.0338],\n",
       "          [ 0.3107,  3.0578],\n",
       "          [-0.2597, -1.3706]], dtype=torch.float64),\n",
       "  tensor([[-1.2085,  0.3836],\n",
       "          [ 0.0170,  1.4365],\n",
       "          [-0.3000, -0.5644],\n",
       "          ...,\n",
       "          [-1.3678, -1.4790],\n",
       "          [ 1.7024,  3.1374],\n",
       "          [-1.7620,  0.3123]], dtype=torch.float64),\n",
       "  tensor([[-2.1859, -2.5797],\n",
       "          [ 0.4709,  2.2084],\n",
       "          [-1.1081,  1.9315],\n",
       "          ...,\n",
       "          [ 2.4824,  1.2614],\n",
       "          [ 0.0337, -2.9134],\n",
       "          [ 0.6340, -0.4434]], dtype=torch.float64),\n",
       "  tensor([[-2.7310, -0.9123],\n",
       "          [-4.3922, -1.5877],\n",
       "          [-0.1134, -2.2829],\n",
       "          ...,\n",
       "          [ 1.8034, -1.0550],\n",
       "          [-0.4977, -0.4455],\n",
       "          [-1.6312,  1.0465]], dtype=torch.float64),\n",
       "  tensor([[-0.0968,  1.3461],\n",
       "          [ 1.4622,  0.6122],\n",
       "          [ 0.8424,  0.2039],\n",
       "          ...,\n",
       "          [ 1.9048, -0.8586],\n",
       "          [ 0.1731, -3.2005],\n",
       "          [-2.0526,  2.7506]], dtype=torch.float64),\n",
       "  tensor([[ 1.0599, -2.8842],\n",
       "          [ 1.1098,  1.0243],\n",
       "          [ 2.9618,  2.3557],\n",
       "          ...,\n",
       "          [ 1.0603,  1.2426],\n",
       "          [-2.6453, -0.9570],\n",
       "          [ 1.5765,  3.2608]], dtype=torch.float64),\n",
       "  tensor([[-0.7866, -2.2043],\n",
       "          [ 3.4518, -0.9615],\n",
       "          [-0.5854, -3.4444],\n",
       "          ...,\n",
       "          [-1.1453,  3.0175],\n",
       "          [ 2.7307, -0.9026],\n",
       "          [ 0.6478, -3.7342]], dtype=torch.float64),\n",
       "  tensor([[-1.8109, -0.3423],\n",
       "          [-1.7519,  2.7243],\n",
       "          [-9.7869, -6.3252],\n",
       "          ...,\n",
       "          [ 2.2823,  0.7735],\n",
       "          [ 1.7670,  0.1151],\n",
       "          [-2.3244,  0.8086]], dtype=torch.float64),\n",
       "  tensor([[  0.4716,  -0.2156],\n",
       "          [  0.6102,   0.4349],\n",
       "          [  0.9339,   0.4385],\n",
       "          ...,\n",
       "          [-37.2488,  43.6203],\n",
       "          [  9.8891, -83.8421],\n",
       "          [ 82.0843,  60.8093]], dtype=torch.float64),\n",
       "  tensor([[-1.9071, -0.7304],\n",
       "          [-1.1440, -0.8250],\n",
       "          [ 2.1193,  0.3021],\n",
       "          ...,\n",
       "          [ 1.5776, -0.8936],\n",
       "          [ 2.6391,  0.5480],\n",
       "          [ 0.1452, -1.3045]], dtype=torch.float64),\n",
       "  tensor([[-1.4613, -0.3211],\n",
       "          [-1.2499,  1.8163],\n",
       "          [ 0.7009, -0.1361],\n",
       "          ...,\n",
       "          [ 0.3427,  0.4758],\n",
       "          [-0.5093, -0.9483],\n",
       "          [ 0.3004,  0.5743]], dtype=torch.float64),\n",
       "  tensor([[-2.7202, -1.2100],\n",
       "          [ 4.1776, -1.9326],\n",
       "          [ 1.2426, -2.2213],\n",
       "          ...,\n",
       "          [ 0.2142, -0.5675],\n",
       "          [ 2.0429, -2.9822],\n",
       "          [-2.2434,  7.6754]], dtype=torch.float64),\n",
       "  tensor([[-5.3136e-01,  8.0530e-01],\n",
       "          [ 1.8500e-02, -2.3830e-01],\n",
       "          [ 3.2582e+00,  5.5668e-02],\n",
       "          ...,\n",
       "          [ 2.8697e+00,  2.0282e+01],\n",
       "          [ 2.9605e+00, -5.4879e+00],\n",
       "          [-1.7835e+01, -1.0146e+01]], dtype=torch.float64),\n",
       "  tensor([[ 0.9742,  0.5249],\n",
       "          [-0.5784,  1.2132],\n",
       "          [-1.2775,  1.8275],\n",
       "          ...,\n",
       "          [ 3.1052, -1.7688],\n",
       "          [-0.1107,  1.2497],\n",
       "          [ 1.6928,  2.1025]], dtype=torch.float64),\n",
       "  tensor([[-1.4696,  0.5550],\n",
       "          [ 0.2035,  0.1546],\n",
       "          [-0.6252, -2.0985],\n",
       "          ...,\n",
       "          [ 1.4061, -0.8787],\n",
       "          [-0.5790,  0.7699],\n",
       "          [ 0.5800,  1.8725]], dtype=torch.float64),\n",
       "  tensor([[-0.5400, -2.6501],\n",
       "          [-6.0001, -6.3008],\n",
       "          [ 1.3644,  6.4185],\n",
       "          ...,\n",
       "          [-1.5332,  1.6837],\n",
       "          [ 2.7415,  0.3047],\n",
       "          [-6.1443,  0.2002]], dtype=torch.float64),\n",
       "  tensor([[-1.5067, -0.2569],\n",
       "          [ 0.4837,  1.7342],\n",
       "          [-2.8409, -2.1945],\n",
       "          ...,\n",
       "          [-0.5493, -0.8087],\n",
       "          [ 0.9761, -0.0921],\n",
       "          [-1.4444, -2.3250]], dtype=torch.float64),\n",
       "  tensor([[ 0.3670,  0.0084],\n",
       "          [ 0.9557, -0.1289],\n",
       "          [ 1.2845, -1.9357],\n",
       "          ...,\n",
       "          [-0.5178, -0.5561],\n",
       "          [ 0.0753, -0.1720],\n",
       "          [-0.3660, -0.5858]], dtype=torch.float64),\n",
       "  tensor([[-9.0391e-01, -8.9748e-01],\n",
       "          [-4.0107e-02, -1.8151e+00],\n",
       "          [ 8.6937e-01, -9.8252e-01],\n",
       "          ...,\n",
       "          [ 1.3821e+01,  5.7562e+01],\n",
       "          [-4.0403e+01, -4.7103e+01],\n",
       "          [ 3.2327e+01,  3.1210e+01]], dtype=torch.float64),\n",
       "  tensor([[-0.0760,  0.2212],\n",
       "          [-4.6659, -2.0919],\n",
       "          [-0.8877, -6.1304],\n",
       "          ...,\n",
       "          [ 0.5181,  1.6833],\n",
       "          [ 0.9409,  5.0567],\n",
       "          [-1.8540, -3.6338]], dtype=torch.float64),\n",
       "  tensor([[-3.8462e+00,  9.7356e-05],\n",
       "          [ 2.4073e+00, -7.4869e-01],\n",
       "          [-1.7909e-01,  1.5896e-02],\n",
       "          ...,\n",
       "          [ 1.9668e+00,  1.8321e-01],\n",
       "          [-4.0878e+00, -1.4182e-01],\n",
       "          [ 7.5298e-01,  1.3559e+00]], dtype=torch.float64),\n",
       "  tensor([[ 1.1133,  3.3655],\n",
       "          [-2.7356, -1.3038],\n",
       "          [ 0.5221,  1.3843],\n",
       "          ...,\n",
       "          [ 1.8785,  0.6188],\n",
       "          [-1.3479,  0.2143],\n",
       "          [-1.2722, -0.5631]], dtype=torch.float64),\n",
       "  tensor([[-0.6551, -0.2760],\n",
       "          [-2.6211,  1.2568],\n",
       "          [ 1.1679, -0.4251],\n",
       "          ...,\n",
       "          [ 0.6646, -2.1754],\n",
       "          [ 0.1188,  0.5105],\n",
       "          [ 1.1459, -0.3892]], dtype=torch.float64),\n",
       "  tensor([[-0.5779,  0.8078],\n",
       "          [-0.2246, -0.0201],\n",
       "          [-0.2078, -2.5083],\n",
       "          ...,\n",
       "          [ 1.4919,  1.6682],\n",
       "          [ 0.2850, -0.8616],\n",
       "          [ 2.6693,  0.2021]], dtype=torch.float64),\n",
       "  tensor([[  3.2323,   1.5300],\n",
       "          [ -2.7990,   0.4202],\n",
       "          [  0.0624,  -0.1069],\n",
       "          ...,\n",
       "          [-20.5630,  38.7145],\n",
       "          [ 31.5470, -34.2527],\n",
       "          [-44.6074,  21.5705]], dtype=torch.float64),\n",
       "  tensor([[-3.4149, -1.2790],\n",
       "          [ 1.1891,  0.3658],\n",
       "          [-0.1661,  1.1350],\n",
       "          ...,\n",
       "          [ 1.0045, -3.7861],\n",
       "          [-1.0639,  2.0914],\n",
       "          [-2.8079, -2.6129]], dtype=torch.float64),\n",
       "  tensor([[ 2.3925,  0.0444],\n",
       "          [ 0.9589, -1.6589],\n",
       "          [-0.2636,  1.7718],\n",
       "          ...,\n",
       "          [-0.5187, -0.3659],\n",
       "          [ 0.2085,  0.0219],\n",
       "          [ 0.0210,  3.4511]], dtype=torch.float64),\n",
       "  tensor([[ 6.9540, -0.9011],\n",
       "          [-5.7753, -1.7448],\n",
       "          [ 4.8829, -6.1335],\n",
       "          ...,\n",
       "          [-1.3044,  0.6693],\n",
       "          [ 4.3510, -1.3542],\n",
       "          [-3.6701,  1.2588]], dtype=torch.float64),\n",
       "  tensor([[ 2.1013,  0.7370],\n",
       "          [-0.7350,  1.7467],\n",
       "          [ 0.9182,  2.1906],\n",
       "          ...,\n",
       "          [ 1.1725, -0.4717],\n",
       "          [-0.0114, -1.6750],\n",
       "          [-0.7892,  5.4703]], dtype=torch.float64),\n",
       "  tensor([[ -0.0495,   1.1251],\n",
       "          [  0.5537,  -0.2186],\n",
       "          [  0.2963,  -0.8458],\n",
       "          ...,\n",
       "          [ -9.3522,  -3.0468],\n",
       "          [ 16.3130,  -1.7801],\n",
       "          [-20.2818,  -1.0256]], dtype=torch.float64),\n",
       "  tensor([[ 1.4594,  1.9215],\n",
       "          [-2.0704,  1.0853],\n",
       "          [ 0.0316, -2.3547],\n",
       "          ...,\n",
       "          [-0.7453, -3.3124],\n",
       "          [ 1.1810, -0.0473],\n",
       "          [ 2.3649, -0.7228]], dtype=torch.float64),\n",
       "  tensor([[ -0.9183,   2.3281],\n",
       "          [  3.1175,  -1.2289],\n",
       "          [  0.2722,  -0.2710],\n",
       "          ...,\n",
       "          [ 11.8157, -12.6644],\n",
       "          [-18.2550,  10.3299],\n",
       "          [ 10.1721,  -0.3711]], dtype=torch.float64),\n",
       "  tensor([[-1.0798,  0.3066],\n",
       "          [ 0.4901, -0.8944],\n",
       "          [-0.0510,  0.9629],\n",
       "          ...,\n",
       "          [-0.4565, -0.3102],\n",
       "          [-0.7056,  0.5715],\n",
       "          [ 0.0822, -0.7126]], dtype=torch.float64),\n",
       "  tensor([[ 0.0056, -2.2951],\n",
       "          [ 1.2765, -2.6822],\n",
       "          [ 0.2113,  2.6033],\n",
       "          ...,\n",
       "          [ 2.2087,  1.7300],\n",
       "          [ 0.2751,  1.1302],\n",
       "          [-2.7728,  0.8796]], dtype=torch.float64),\n",
       "  tensor([[ 0.2362,  0.2518],\n",
       "          [ 1.2322, -2.5008],\n",
       "          [ 3.3377, -0.8040],\n",
       "          ...,\n",
       "          [-2.5589, -1.0336],\n",
       "          [-1.6034,  1.9240],\n",
       "          [ 4.0534, -1.8205]], dtype=torch.float64),\n",
       "  tensor([[ 0.4461,  2.7690],\n",
       "          [-0.0756,  1.2903],\n",
       "          [ 2.6821, -0.3562],\n",
       "          ...,\n",
       "          [-4.4024, -0.0513],\n",
       "          [-2.3098, -2.7088],\n",
       "          [-1.8111, -0.4498]], dtype=torch.float64),\n",
       "  tensor([[ 1.1133,  5.7275],\n",
       "          [ 1.6892,  0.6822],\n",
       "          [ 4.9664,  4.5134],\n",
       "          ...,\n",
       "          [-0.5681,  1.5773],\n",
       "          [ 2.7027, -0.3998],\n",
       "          [ 0.7380,  4.7622]], dtype=torch.float64),\n",
       "  tensor([[ 0.5210,  0.1812],\n",
       "          [ 1.6025, -1.7436],\n",
       "          [ 0.2271,  2.0717],\n",
       "          ...,\n",
       "          [-1.6831, -0.7110],\n",
       "          [-1.5038, -4.0034],\n",
       "          [ 1.2211, -0.0647]], dtype=torch.float64),\n",
       "  tensor([[  1.7218,   1.6355],\n",
       "          [ -0.4424,  -0.9968],\n",
       "          [ -2.2185,  -1.0192],\n",
       "          ...,\n",
       "          [-39.3315, -35.1466],\n",
       "          [ 17.9989,  21.4752],\n",
       "          [-14.4397,  -2.1799]], dtype=torch.float64),\n",
       "  tensor([[ 3.0293,  1.4373],\n",
       "          [ 0.0368, -1.0963],\n",
       "          [-0.7940,  0.0249],\n",
       "          ...,\n",
       "          [ 0.2148, -1.0777],\n",
       "          [ 2.5050,  0.3602],\n",
       "          [-0.9217, -0.0352]], dtype=torch.float64),\n",
       "  tensor([[-2.2405, -1.0897],\n",
       "          [ 0.0552,  1.7483],\n",
       "          [-0.4006, -2.1923],\n",
       "          ...,\n",
       "          [-1.4389, -2.8956],\n",
       "          [ 0.4232,  1.5913],\n",
       "          [-0.8110, -0.2109]], dtype=torch.float64),\n",
       "  tensor([[ -1.0057,  -0.8762],\n",
       "          [ -0.4480,   1.2662],\n",
       "          [  1.1072,  -0.1497],\n",
       "          ...,\n",
       "          [-25.4034,  23.6181],\n",
       "          [ 32.0370, -23.3399],\n",
       "          [-25.4648,  19.2760]], dtype=torch.float64),\n",
       "  tensor([[ 0.2980,  0.6368],\n",
       "          [ 0.7048, -0.4338],\n",
       "          [-0.3612,  1.1707],\n",
       "          ...,\n",
       "          [-2.8894, -0.9507],\n",
       "          [ 0.2524,  2.1692],\n",
       "          [-0.1056, -1.9672]], dtype=torch.float64),\n",
       "  tensor([[ 0.2726, -2.9469],\n",
       "          [ 2.0692,  1.3596],\n",
       "          [-2.1866, -1.0820],\n",
       "          ...,\n",
       "          [-5.1488,  3.8997],\n",
       "          [ 4.2573, -3.1627],\n",
       "          [-0.1137, -1.7846]], dtype=torch.float64),\n",
       "  tensor([[-0.8587,  1.4428],\n",
       "          [-0.5460,  0.5298],\n",
       "          [ 2.7273,  1.1901],\n",
       "          ...,\n",
       "          [ 0.7235, -0.9684],\n",
       "          [ 0.3482, -0.8098],\n",
       "          [ 1.5559,  0.4081]], dtype=torch.float64),\n",
       "  tensor([[ 5.0198,  2.4892],\n",
       "          [-5.4888, -1.5549],\n",
       "          [-0.9396, -1.3375],\n",
       "          ...,\n",
       "          [-3.0716,  1.1669],\n",
       "          [ 1.7468,  1.8142],\n",
       "          [-3.1930, -8.6978]], dtype=torch.float64),\n",
       "  tensor([[ 1.7544, -1.0383],\n",
       "          [-3.6205, -2.3655],\n",
       "          [-0.3607, -1.2445],\n",
       "          ...,\n",
       "          [ 1.3837,  1.0427],\n",
       "          [-1.3920, -1.3301],\n",
       "          [-0.6340, -0.8163]], dtype=torch.float64),\n",
       "  tensor([[ 2.3283,  1.2665],\n",
       "          [-3.3985,  0.6665],\n",
       "          [-1.5685,  0.0464],\n",
       "          ...,\n",
       "          [-0.7895,  1.2132],\n",
       "          [ 0.0533, -0.0333],\n",
       "          [ 1.1024, -0.6273]], dtype=torch.float64),\n",
       "  tensor([[-1.9123,  1.4978],\n",
       "          [ 7.7116, -2.2878],\n",
       "          [-3.6725,  2.5982],\n",
       "          ...,\n",
       "          [ 2.0589,  0.0895],\n",
       "          [-0.3615, -2.1204],\n",
       "          [-3.4066, -2.1417]], dtype=torch.float64),\n",
       "  tensor([[-1.3107, -0.4710],\n",
       "          [-1.0221, -0.8945],\n",
       "          [-0.4563,  0.5238],\n",
       "          ...,\n",
       "          [ 1.3151,  7.1874],\n",
       "          [-1.2597, -1.8825],\n",
       "          [-0.9629,  2.4708]], dtype=torch.float64),\n",
       "  tensor([[-1.5246,  1.9256],\n",
       "          [ 0.6213,  2.9187],\n",
       "          [ 5.7719, -0.5557],\n",
       "          ...,\n",
       "          [-3.2606, -4.1228],\n",
       "          [ 4.2191, -0.9944],\n",
       "          [-1.5266,  3.9006]], dtype=torch.float64),\n",
       "  tensor([[ 0.1983, -1.9318],\n",
       "          [-0.0988, -0.1173],\n",
       "          [-1.4440, -0.1069],\n",
       "          ...,\n",
       "          [ 2.5859,  0.5477],\n",
       "          [-0.4646,  1.9239],\n",
       "          [ 2.2624,  0.5403]], dtype=torch.float64)],\n",
       " [tensor([68, 64, 37,  ..., 53, 56, 61]),\n",
       "  tensor([ 33,  40,  33,  ...,  99, 100, 104]),\n",
       "  tensor([57, 50, 38,  ..., 66, 69, 74]),\n",
       "  tensor([41, 45, 48,  ..., 45, 53, 29]),\n",
       "  tensor([67, 55, 74,  ..., 58, 46, 34]),\n",
       "  tensor([69, 57, 71,  ..., 50, 54, 38]),\n",
       "  tensor([79, 48, 48,  ..., 67, 72, 36]),\n",
       "  tensor([67, 67, 75,  ..., 39, 43, 75]),\n",
       "  tensor([64, 73, 69,  ..., 35, 47, 71]),\n",
       "  tensor([65, 65, 63,  ..., 70, 77, 74]),\n",
       "  tensor([48, 36, 51,  ..., 55, 31, 43]),\n",
       "  tensor([37, 40, 45,  ..., 45, 65, 69]),\n",
       "  tensor([77, 77, 76,  ..., 57, 62, 54]),\n",
       "  tensor([67, 67, 72,  ..., 79, 76, 84]),\n",
       "  tensor([70, 42, 49,  ..., 66, 30, 37]),\n",
       "  tensor([58, 63, 27,  ..., 51, 39, 63]),\n",
       "  tensor([66, 30, 42,  ..., 64, 67, 72]),\n",
       "  tensor([68, 60, 51,  ..., 56, 60, 68]),\n",
       "  tensor([66, 78, 74,  ..., 59, 62, 66]),\n",
       "  tensor([72, 51, 44,  ..., 44, 63, 68]),\n",
       "  tensor([66, 54, 54,  ..., 62, 67, 50]),\n",
       "  tensor([43, 67, 67,  ..., 59, 62, 43]),\n",
       "  tensor([54, 81, 61,  ..., 45, 42, 38]),\n",
       "  tensor([82, 58, 62,  ..., 82, 58, 70]),\n",
       "  tensor([58, 58, 58,  ..., 30, 25, 24]),\n",
       "  tensor([57, 58, 57,  ..., 58, 50, 70]),\n",
       "  tensor([78, 49, 78,  ..., 48, 53, 57]),\n",
       "  tensor([63, 75, 68,  ..., 84, 72, 80]),\n",
       "  tensor([68, 51, 60,  ..., 56, 60, 68]),\n",
       "  tensor([27, 39, 27,  ..., 70, 70, 63]),\n",
       "  tensor([86, 50, 38,  ..., 54, 57, 50]),\n",
       "  tensor([34, 41, 46,  ..., 38, 41, 38]),\n",
       "  tensor([44, 63, 51,  ..., 74, 76, 68]),\n",
       "  tensor([67, 63, 48,  ..., 39, 51, 75]),\n",
       "  tensor([59, 68, 63,  ..., 83, 92, 92]),\n",
       "  tensor([67, 55, 55,  ..., 46, 34, 58]),\n",
       "  tensor([55, 55, 56,  ..., 39, 51, 27]),\n",
       "  tensor([71, 59, 67,  ..., 71, 79, 83]),\n",
       "  tensor([75, 75, 32,  ..., 42, 66, 58]),\n",
       "  tensor([43, 50, 50,  ..., 24, 60, 52]),\n",
       "  tensor([40, 40, 47,  ..., 67, 59, 64]),\n",
       "  tensor([65, 63, 65,  ..., 65, 64, 64]),\n",
       "  tensor([81, 81, 83,  ..., 62, 62, 74]),\n",
       "  tensor([39, 40, 39,  ..., 35, 33, 57]),\n",
       "  tensor([78, 66, 78,  ..., 52, 61, 49]),\n",
       "  tensor([81, 76, 69,  ..., 91, 87, 87]),\n",
       "  tensor([74, 74, 79,  ..., 67, 55, 43]),\n",
       "  tensor([69, 69, 66,  ..., 42, 93, 99]),\n",
       "  tensor([48, 36, 51,  ..., 43, 55, 31]),\n",
       "  tensor([72, 41, 68,  ..., 87, 32, 92]),\n",
       "  tensor([38, 45, 38,  ..., 89, 86, 77]),\n",
       "  tensor([69, 57, 69,  ..., 26, 32, 33]),\n",
       "  tensor([67, 63, 65,  ..., 55, 63, 58]),\n",
       "  tensor([60, 55, 51,  ..., 63, 67, 72]),\n",
       "  tensor([73, 56, 53,  ..., 67, 82, 72]),\n",
       "  tensor([39, 51, 63,  ..., 63, 66, 83]),\n",
       "  tensor([74, 67, 71,  ..., 59, 50, 67]),\n",
       "  tensor([47, 55, 64,  ..., 55, 28, 35]),\n",
       "  tensor([59, 47, 61,  ..., 52, 28, 40]),\n",
       "  tensor([75, 48, 63,  ..., 36, 48, 60]),\n",
       "  tensor([55, 43, 31,  ..., 67, 72, 76]),\n",
       "  tensor([67, 63, 63,  ..., 63, 46, 51]),\n",
       "  tensor([68, 37, 64,  ..., 53, 56, 61]),\n",
       "  tensor([60, 65, 60,  ..., 41, 60, 53]),\n",
       "  tensor([79, 67, 70,  ..., 63, 55, 58]),\n",
       "  tensor([65, 63, 65,  ..., 82, 77, 74]),\n",
       "  tensor([50, 50, 55,  ..., 57, 54, 62]),\n",
       "  tensor([68, 60, 44,  ..., 48, 60, 68]),\n",
       "  tensor([72, 63, 60,  ..., 75, 79, 82]),\n",
       "  tensor([88, 76, 88,  ..., 55, 57, 55]),\n",
       "  tensor([67, 67, 65,  ..., 97, 94, 37]),\n",
       "  tensor([70, 50, 46,  ..., 50, 53, 58]),\n",
       "  tensor([77, 46, 74,  ..., 70, 74, 82]),\n",
       "  tensor([72, 72, 70,  ..., 53, 29, 41]),\n",
       "  tensor([75, 63, 68,  ..., 92, 80, 84]),\n",
       "  tensor([79, 48, 48,  ..., 40, 36, 36]),\n",
       "  tensor([64, 61, 45,  ..., 66, 45, 66]),\n",
       "  tensor([71, 71, 71,  ..., 55, 67, 62]),\n",
       "  tensor([67, 67, 65,  ..., 27, 51, 39])])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i = 0\n",
    "#for batch in train_iter:\n",
    "#    b = batch\n",
    "#    i += 1\n",
    "#    if i > 1: \n",
    "#        break\n",
    "\n",
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84f7cd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>ticks_per_beat</th>\n",
       "      <th>mid</th>\n",
       "      <th>midi_notes</th>\n",
       "      <th>midi_velocity</th>\n",
       "      <th>midi_time</th>\n",
       "      <th>sampling_frequency</th>\n",
       "      <th>sym13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004</td>\n",
       "      <td>480</td>\n",
       "      <td>MidiFile(type=1, ticks_per_beat=480, tracks=[\\...</td>\n",
       "      <td>[71, 71, 55, 71, 59, 55, 59, 62, 62, 72, 71, 6...</td>\n",
       "      <td>[60, 0, 44, 54, 55, 0, 0, 52, 0, 76, 0, 56, 0,...</td>\n",
       "      <td>[0.13333333333333333, 0.0010416666666666667, 0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>[[6.6829083086832854, 1.2684792957869424], [-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004</td>\n",
       "      <td>480</td>\n",
       "      <td>MidiFile(type=1, ticks_per_beat=480, tracks=[\\...</td>\n",
       "      <td>[43, 31, 31, 43, 44, 32, 46, 34, 44, 32, 34, 4...</td>\n",
       "      <td>[43, 36, 0, 0, 45, 35, 55, 38, 0, 0, 0, 0, 56,...</td>\n",
       "      <td>[1.0322916666666666, 0.008333333333333333, 0.0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>[[-2.3087699720057997, 2.7493433056420518], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004</td>\n",
       "      <td>480</td>\n",
       "      <td>MidiFile(type=1, ticks_per_beat=480, tracks=[\\...</td>\n",
       "      <td>[38, 45, 38, 45, 53, 53, 62, 62, 74, 67, 53, 6...</td>\n",
       "      <td>[37, 41, 0, 0, 53, 0, 58, 0, 46, 44, 38, 0, 59...</td>\n",
       "      <td>[0.041666666666666664, 0.11458333333333333, 0....</td>\n",
       "      <td>44100</td>\n",
       "      <td>[[-0.577860502384477, 0.8078359385863222], [-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004</td>\n",
       "      <td>480</td>\n",
       "      <td>MidiFile(type=1, ticks_per_beat=480, tracks=[\\...</td>\n",
       "      <td>[55, 43, 31, 31, 43, 55, 62, 50, 55, 77, 71, 8...</td>\n",
       "      <td>[97, 100, 90, 0, 0, 0, 93, 89, 85, 90, 96, 92,...</td>\n",
       "      <td>[0.029166666666666667, 0.0020833333333333333, ...</td>\n",
       "      <td>44100</td>\n",
       "      <td>[[0.00558889660656537, -2.295053071159729], [1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>480</td>\n",
       "      <td>MidiFile(type=1, ticks_per_beat=480, tracks=[\\...</td>\n",
       "      <td>[32, 75, 32, 51, 68, 51, 75, 72, 56, 68, 56, 6...</td>\n",
       "      <td>[19, 45, 0, 25, 21, 0, 0, 27, 40, 0, 0, 29, 32...</td>\n",
       "      <td>[0.9864583333333333, 0.196875, 0.04375, 0.4104...</td>\n",
       "      <td>44100</td>\n",
       "      <td>[[0.1594345508771419, 0.5193846127925084], [2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2004</td>\n",
       "      <td>480</td>\n",
       "      <td>MidiFile(type=1, ticks_per_beat=480, tracks=[\\...</td>\n",
       "      <td>[67, 65, 67, 70, 65, 72, 70, 77, 72, 74, 77, 7...</td>\n",
       "      <td>[89, 88, 0, 85, 0, 82, 0, 82, 0, 84, 0, 81, 0,...</td>\n",
       "      <td>[1.078125, 0.01875, 0.0010416666666666667, 0.0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>[[-0.8336377828470809, -0.4952179091205502], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2004</td>\n",
       "      <td>480</td>\n",
       "      <td>MidiFile(type=1, ticks_per_beat=480, tracks=[\\...</td>\n",
       "      <td>[60, 36, 48, 51, 55, 51, 48, 60, 55, 36, 60, 5...</td>\n",
       "      <td>[93, 83, 80, 81, 82, 0, 0, 0, 0, 0, 80, 75, 76...</td>\n",
       "      <td>[0.596875, 0.005208333333333333, 0.00729166666...</td>\n",
       "      <td>44100</td>\n",
       "      <td>[[-0.6426064856535821, -0.1041292077125284], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2004</td>\n",
       "      <td>480</td>\n",
       "      <td>MidiFile(type=1, ticks_per_beat=480, tracks=[\\...</td>\n",
       "      <td>[68, 60, 51, 44, 51, 60, 44, 68, 68, 61, 51, 4...</td>\n",
       "      <td>[58, 41, 28, 35, 0, 0, 0, 0, 61, 43, 32, 38, 6...</td>\n",
       "      <td>[1.003125, 0.053125, 0.007291666666666667, 0.0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>[[2.0107936641664907, -0.3399585682533319], [-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2004</td>\n",
       "      <td>480</td>\n",
       "      <td>MidiFile(type=1, ticks_per_beat=480, tracks=[\\...</td>\n",
       "      <td>[75, 48, 63, 63, 55, 75, 77, 55, 65, 50, 48, 6...</td>\n",
       "      <td>[52, 27, 31, 0, 35, 0, 66, 0, 30, 44, 0, 0, 37...</td>\n",
       "      <td>[0.03958333333333333, 0.035416666666666666, 0....</td>\n",
       "      <td>44100</td>\n",
       "      <td>[[-1.0797728625021672, 0.3066298351203736], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2004</td>\n",
       "      <td>480</td>\n",
       "      <td>MidiFile(type=1, ticks_per_beat=480, tracks=[\\...</td>\n",
       "      <td>[67, 67, 75, 48, 75, 48, 51, 51, 55, 75, 74, 5...</td>\n",
       "      <td>[49, 0, 55, 36, 0, 0, 48, 0, 43, 62, 69, 0, 0,...</td>\n",
       "      <td>[0.20520833333333333, 0.0625, 0.025, 0.0125, 0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>[[-0.19821113196514278, 0.8984327068144266], [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  ticks_per_beat                                                mid  \\\n",
       "0    2004             480  MidiFile(type=1, ticks_per_beat=480, tracks=[\\...   \n",
       "1    2004             480  MidiFile(type=1, ticks_per_beat=480, tracks=[\\...   \n",
       "2    2004             480  MidiFile(type=1, ticks_per_beat=480, tracks=[\\...   \n",
       "3    2004             480  MidiFile(type=1, ticks_per_beat=480, tracks=[\\...   \n",
       "4    2004             480  MidiFile(type=1, ticks_per_beat=480, tracks=[\\...   \n",
       "..    ...             ...                                                ...   \n",
       "127  2004             480  MidiFile(type=1, ticks_per_beat=480, tracks=[\\...   \n",
       "128  2004             480  MidiFile(type=1, ticks_per_beat=480, tracks=[\\...   \n",
       "129  2004             480  MidiFile(type=1, ticks_per_beat=480, tracks=[\\...   \n",
       "130  2004             480  MidiFile(type=1, ticks_per_beat=480, tracks=[\\...   \n",
       "131  2004             480  MidiFile(type=1, ticks_per_beat=480, tracks=[\\...   \n",
       "\n",
       "                                            midi_notes  \\\n",
       "0    [71, 71, 55, 71, 59, 55, 59, 62, 62, 72, 71, 6...   \n",
       "1    [43, 31, 31, 43, 44, 32, 46, 34, 44, 32, 34, 4...   \n",
       "2    [38, 45, 38, 45, 53, 53, 62, 62, 74, 67, 53, 6...   \n",
       "3    [55, 43, 31, 31, 43, 55, 62, 50, 55, 77, 71, 8...   \n",
       "4    [32, 75, 32, 51, 68, 51, 75, 72, 56, 68, 56, 6...   \n",
       "..                                                 ...   \n",
       "127  [67, 65, 67, 70, 65, 72, 70, 77, 72, 74, 77, 7...   \n",
       "128  [60, 36, 48, 51, 55, 51, 48, 60, 55, 36, 60, 5...   \n",
       "129  [68, 60, 51, 44, 51, 60, 44, 68, 68, 61, 51, 4...   \n",
       "130  [75, 48, 63, 63, 55, 75, 77, 55, 65, 50, 48, 6...   \n",
       "131  [67, 67, 75, 48, 75, 48, 51, 51, 55, 75, 74, 5...   \n",
       "\n",
       "                                         midi_velocity  \\\n",
       "0    [60, 0, 44, 54, 55, 0, 0, 52, 0, 76, 0, 56, 0,...   \n",
       "1    [43, 36, 0, 0, 45, 35, 55, 38, 0, 0, 0, 0, 56,...   \n",
       "2    [37, 41, 0, 0, 53, 0, 58, 0, 46, 44, 38, 0, 59...   \n",
       "3    [97, 100, 90, 0, 0, 0, 93, 89, 85, 90, 96, 92,...   \n",
       "4    [19, 45, 0, 25, 21, 0, 0, 27, 40, 0, 0, 29, 32...   \n",
       "..                                                 ...   \n",
       "127  [89, 88, 0, 85, 0, 82, 0, 82, 0, 84, 0, 81, 0,...   \n",
       "128  [93, 83, 80, 81, 82, 0, 0, 0, 0, 0, 80, 75, 76...   \n",
       "129  [58, 41, 28, 35, 0, 0, 0, 0, 61, 43, 32, 38, 6...   \n",
       "130  [52, 27, 31, 0, 35, 0, 66, 0, 30, 44, 0, 0, 37...   \n",
       "131  [49, 0, 55, 36, 0, 0, 48, 0, 43, 62, 69, 0, 0,...   \n",
       "\n",
       "                                             midi_time  sampling_frequency  \\\n",
       "0    [0.13333333333333333, 0.0010416666666666667, 0...               44100   \n",
       "1    [1.0322916666666666, 0.008333333333333333, 0.0...               44100   \n",
       "2    [0.041666666666666664, 0.11458333333333333, 0....               44100   \n",
       "3    [0.029166666666666667, 0.0020833333333333333, ...               44100   \n",
       "4    [0.9864583333333333, 0.196875, 0.04375, 0.4104...               44100   \n",
       "..                                                 ...                 ...   \n",
       "127  [1.078125, 0.01875, 0.0010416666666666667, 0.0...               44100   \n",
       "128  [0.596875, 0.005208333333333333, 0.00729166666...               44100   \n",
       "129  [1.003125, 0.053125, 0.007291666666666667, 0.0...               44100   \n",
       "130  [0.03958333333333333, 0.035416666666666666, 0....               44100   \n",
       "131  [0.20520833333333333, 0.0625, 0.025, 0.0125, 0...               44100   \n",
       "\n",
       "                                                 sym13  \n",
       "0    [[6.6829083086832854, 1.2684792957869424], [-0...  \n",
       "1    [[-2.3087699720057997, 2.7493433056420518], [0...  \n",
       "2    [[-0.577860502384477, 0.8078359385863222], [-0...  \n",
       "3    [[0.00558889660656537, -2.295053071159729], [1...  \n",
       "4    [[0.1594345508771419, 0.5193846127925084], [2....  \n",
       "..                                                 ...  \n",
       "127  [[-0.8336377828470809, -0.4952179091205502], [...  \n",
       "128  [[-0.6426064856535821, -0.1041292077125284], [...  \n",
       "129  [[2.0107936641664907, -0.3399585682533319], [-...  \n",
       "130  [[-1.0797728625021672, 0.3066298351203736], [0...  \n",
       "131  [[-0.19821113196514278, 0.8984327068144266], [...  \n",
       "\n",
       "[132 rows x 8 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ff14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c52adbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(132, 128)\n",
       "    (rnn): LSTM(128, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(132, 128)\n",
       "    (rnn): LSTM(128, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=132, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed47a713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,933,636 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4709b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4d5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator:DataLoader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[0]\n",
    "        trg = batch[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # trg = [sen_len, batch_size]\n",
    "        # output = [trg_len, batch_size, output_dim]\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # transfrom our output : slice off the first column, and flatten the output into 2 dim.\n",
    "        output = output[1:].view(-1, output_dim) \n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg_len-1) * batch_size]\n",
    "        # output = [(trg_len-1) * batch_size, output_dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "580547f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            \n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "            \n",
    "            output = model(src, trg, 0) # turn off teacher forcing.\n",
    "            \n",
    "            # trg = [sen_len, batch_size]\n",
    "            # output = [sen_len, batch_size, output_dim]\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec0bb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that used to tell us how long an epoch takes.\n",
    "def epoch_time(start_time, end_time):\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time  / 60)\n",
    "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
    "    return  elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec523434",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[1;32m      9\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_iter, optimizer, criterion, CLIP)\n\u001b[1;32m     12\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_iter, criterion)\n\u001b[1;32m     14\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m \u001b[39m# trg = [sen_len, batch_size]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# output = [trg_len, batch_size, output_dim]\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m output \u001b[39m=\u001b[39m model(src, trg)\n\u001b[1;32m     15\u001b[0m output_dim \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[39m# transfrom our output : slice off the first column, and flatten the output into 2 dim.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavaetro--xUrs-Fb-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     23\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(trg_len, batch_size, trg_vocab_size)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     24\u001b[0m \u001b[39m# ICI ça passe\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[39m# last hidden state of the encoder is used as the initial hidden state of the decoder\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m hidden, cell \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src)\n\u001b[1;32m     29\u001b[0m \u001b[39m# first input to the decoder is the <sos> token.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m trg[\u001b[39m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavaetro--xUrs-Fb-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src):\n\u001b[1;32m     17\u001b[0m     \u001b[39m# src : [sen_len, batch_size]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(src))\n\u001b[1;32m     20\u001b[0m     \u001b[39m# embedded : [sen_len, batch_size, emb_dim]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     outputs, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(embedded)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavaetro--xUrs-Fb-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavaetro--xUrs-Fb-py3.11/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/wavaetro--xUrs-Fb-py3.11/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'Seq2SeqModel.pt')\n",
    "    print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
